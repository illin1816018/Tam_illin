{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc80WxtqX-e9"
      },
      "source": [
        "This notebook preprocess ensemble GEFS according to input used in Price and Rasp (2022)\n",
        "\n",
        "Input data is 5 ensemble apcp, 5 ensemble pwat, cape, cin, and t2m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "179t4teuX-e_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a24548-0df9-4847-9a8f-df17923bcc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.10/dist-packages (1.6.4)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.6.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4) (2023.7.22)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.23.5)\n",
            "[40.9517 39.047  37.1422]\n"
          ]
        }
      ],
      "source": [
        "!pip install netCDF4\n",
        "import xarray as xr\n",
        "from netCDF4 import Dataset\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# import cartopy.crs as ccrs  # for plotting map\n",
        "# import cartopy\n",
        "import matplotlib as mpl\n",
        "import requests\n",
        "a = Dataset(\"finalprc.nc\")\n",
        "b = a.variables['lat'][:]\n",
        "print(b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "1DI4fTjlI256",
        "outputId": "13f80f8e-8d5a-4393-e27b-0704bea42b93"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: [<class 'h5netcdf.core.File'>, ('/content/finalhgt850.nc',), 'r', (('decode_vlen_strings', True), ('invalid_netcdf', None)), 'cae71876-2db2-4fff-b758-7887d10b8751']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5e66734c5097>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load your xarray dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finalhgt850.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert numpy.datetime64 to '2000-01-01T06:00' format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0moverwrite_encoded_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite_encoded_chunks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     backend_ds = backend.open_dataset(\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/h5netcdf_.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, format, group, lock, invalid_netcdf, phony_dims, decode_vlen_strings)\u001b[0m\n\u001b[1;32m    414\u001b[0m     ) -> Dataset:\n\u001b[1;32m    415\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         store = H5NetCDFStore.open(\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/h5netcdf_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, lock, autoclose, invalid_netcdf, phony_dims, decode_vlen_strings)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCachingFileManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5netcdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/h5netcdf_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# todo: utilizing find_root_and_group seems a bit clunky\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m#  making filename available on h5netcdf.Group seems better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_root_and_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/h5netcdf_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/h5netcdf_.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             ds = _nc4_require_group(\n\u001b[1;32m    182\u001b[0m                 \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_h5netcdf_create_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_with_cache_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0;31m# ensure file doesn't get overridden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5netcdf/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, invalid_netcdf, phony_dims, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preexisting_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_h5py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                     self._h5file = self._h5py.File(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/content/finalhgt850.nc', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "import xarray as xr\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load your xarray dataset\n",
        "rain = xr.open_dataset('finalhgt850.nc')\n",
        "\n",
        "# Convert numpy.datetime64 to '2000-01-01T06:00' format\n",
        "def convert_datetime64_to_format(dt64):\n",
        "    target_date = (dt64 - np.datetime64('1800-01-01')) / np.timedelta64(1, 'h')\n",
        "    return (datetime(1800, 1, 1) + timedelta(hours=target_date)).strftime('%Y-%m-%dT%H:%M')\n",
        "\n",
        "# Convert the time coordinates to the desired format\n",
        "new_time_coords = [convert_datetime64_to_format(dt64) for dt64 in rain.time.values]\n",
        "\n",
        "# Replace the existing time coordinate with the new formatted time coordinates\n",
        "rain = rain.assign_coords(time=new_time_coords)\n",
        "\n",
        "print(rain)\n",
        "a = rain.squeeze('level')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K8jrhMBzUF2H",
        "outputId": "962fa470-f653-4763-aa95-511ff091dbd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<xarray.Dataset>\n",
            "Dimensions:  (time: 45292, lon: 3, lat: 2)\n",
            "Coordinates:\n",
            "  * time     (time) datetime64[ns] 1980-01-01 ... 2010-12-31T18:00:00\n",
            "  * lon      (lon) float32 280.0 282.5 285.0\n",
            "  * lat      (lat) float32 40.0 37.5\n",
            "    level    float32 850.0\n",
            "Data variables:\n",
            "    shum     (time, lat, lon) float32 ...\n",
            "Attributes:\n",
            "    CDI:            Climate Data Interface version 1.9.10 (https://mpimet.mpg...\n",
            "    Conventions:    COARDS\n",
            "    title:          4x daily NMC reanalysis (1980)\n",
            "    description:    Data is from NMC initialized reanalysis\\n(4x/day).  It co...\n",
            "    platform:       Model\n",
            "    dataset_title:  NCEP-NCAR Reanalysis 1\n",
            "    References:     http://www.psl.noaa.gov/data/gridded/data.ncep.reanalysis...\n",
            "    CDO:            Climate Data Operators version 1.9.10 (https://mpimet.mpg...\n",
            "    history:        07-Aug-2023 18:45:15 Subset NOAA/PSL  \n"
          ]
        }
      ],
      "source": [
        "hgt850 = xr.open_dataset(\"finalhgt850.nc\")\n",
        "hgt500 = xr.open_dataset(\"finalhgt500.nc\")\n",
        "rhum = xr.open_dataset(\"finalrhum.nc\")\n",
        "shum = xr.open_dataset(\"finalshum.nc\")\n",
        "temp = xr.open_dataset(\"finaltemp.nc\")\n",
        "slp = xr.open_dataset(\"finalmslp.nc\")\n",
        "\n",
        "hgt500 = hgt500.squeeze('level')\n",
        "hgt850 = hgt850.squeeze(\"level\")\n",
        "shum = shum.squeeze(\"level\")\n",
        "print(shum)\n",
        "final = [hgt850, hgt500, rhum, shum, temp, slp]\n",
        "# for i in final:\n",
        "#     # Convert the time coordinates to the desired format\n",
        "#     new_time_coords = [convert_datetime64_to_format(dt64) for dt64 in i.time.values]\n",
        "\n",
        "# # Replace the existing time coordinate with the new formatted time coordinates\n",
        "#     i = rain.assign_coords(time=new_time_coords)\n",
        "# print(hgt850)\n",
        "\n",
        "\n",
        "def trainvaltest(dataset):\n",
        "    '''\n",
        "    Return train, val, test dataset\n",
        "    '''\n",
        "    train = dataset.sel(time=slice('1980-01-01T06', '2005-01-01T06'))\n",
        "    val = dataset.sel(time=slice('2006-01-01T12', '2008-12-31T12'))\n",
        "    test = dataset.sel(time=slice('2009-01-01T18', '2010-12-31T18'))\n",
        "    return train, val, test\n",
        "\n",
        "t,v,te = trainvaltest(rain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aj8VG3VBX-fC"
      },
      "outputs": [],
      "source": [
        "# print(len(set(cin_c00.time.values)))\n",
        "# print(len(cin_c00.time.values))\n",
        "# print(len(set(pwat_p01.time.values)))\n",
        "# print(len(pwat_p01.time.values))\n",
        "\n",
        "# have duplicated dates in pwat_p01, p02, p03, p04\n",
        "# print(len(set(pwat_p01.time.values)))\n",
        "# print(len(pwat_p01.time.values))\n",
        "\n",
        "# to find the indices of duplicated value\n",
        "# pwat_duplicates = list(pwat_p01.get_index(\"time\").duplicated())\n",
        "# indices = [i for i, x in enumerate(pwat_duplicates) if x == True]\n",
        "# indices\n",
        "\n",
        "# check if duplicates are due to error in date naming\n",
        "# pwat_p01.isel(time=6200).pwat.values\n",
        "# pwat_p01.isel(time=9116).pwat.values\n",
        "# the duplicates are exactly the same, so it is not a date naming error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NiiJpnizEfe0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ePRSevQvLPP7"
      },
      "outputs": [],
      "source": [
        "hgt850_train, hgt850_val, hgt850_test = trainvaltest(hgt850)\n",
        "hgt500_train, hgt500_val, hgt500_test = trainvaltest(hgt500)\n",
        "rhum_train, rhum_val, rhum_test = trainvaltest(rhum)\n",
        "shum_train, shum_val, shum_test = trainvaltest(shum)\n",
        "slp_train, slp_val, slp_test = trainvaltest(slp)\n",
        "temp_train, temp_val, temp_test = trainvaltest(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9oWY0vg3FnaQ"
      },
      "outputs": [],
      "source": [
        "def resampling(partial_dataset, var):\n",
        "    partial_dataset = partial_dataset.resample(time='6H').asfreq()\n",
        "    if var == 'hgt':\n",
        "        partial_dataset.hgt.values = partial_dataset.hgt.interpolate_na(dim='time')\n",
        "        return partial_dataset\n",
        "    if var == 'slp':\n",
        "        partial_dataset.slp.values = partial_dataset.slp.interpolate_na(dim='time')\n",
        "        return partial_dataset\n",
        "    if var == 'rhum':\n",
        "        partial_dataset.rhum.values = partial_dataset.rhum.interpolate_na(dim='time')\n",
        "        return partial_dataset\n",
        "    if var == 'shum':\n",
        "        partial_dataset.shum.values = partial_dataset.shum.interpolate_na(dim='time')\n",
        "        return partial_dataset\n",
        "    if var == 'tmp':\n",
        "        partial_dataset.tmp.values = partial_dataset.tmp.interpolate_na(dim='time')\n",
        "        return partial_dataset\n",
        "    else:\n",
        "        print('Error: var not found in function, please define')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aPLI50pOsqj"
      },
      "outputs": [],
      "source": [
        "hgt850_train = resampling(hgt850_train, 'hgt')\n",
        "hgt500_train = resampling(hgt500_train, 'hgt')\n",
        "rhum_train = resampling(rhum_train, 'rhum')\n",
        "shum_train = resampling(shum_train, 'shum')\n",
        "slp_train = resampling(slp_train, 'slp')\n",
        "temp_train = resampling(temp_train, 'tmp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvIbMZq7X-fD"
      },
      "outputs": [],
      "source": [
        "# import scipy as sp\n",
        "# # a=cape_train_c00.cape.values\n",
        "# # a = pwat_train_c00.pwat.values\n",
        "\n",
        "# a=t2m_train_c00.t2m.values\n",
        "\n",
        "# p=sp.stats.mstats.normaltest(a, axis=0).pvalue\n",
        "# if p.all()<0.01:\n",
        "#    print ('distribution is not normal')\n",
        "# p=sp.stats.mstats.normaltest(np.log(a), axis=0).pvalue\n",
        "# if p.all()<0.01:\n",
        "#    print ('distribution is not log-normal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZPYWp42X-fD"
      },
      "outputs": [],
      "source": [
        "# ALL MISSING DATES LIE IN TRAINING DATASET\n",
        "\n",
        "# print(timecheck(apcp_train_p01, apcp_val_p01, apcp_test_p01))\n",
        "# print(timecheck(apcp_train_p02, apcp_val_p02, apcp_test_p02))\n",
        "# print(timecheck(apcp_train_p03, apcp_val_p03, apcp_test_p03))\n",
        "# print(timecheck(apcp_train_p04, apcp_val_p04, apcp_test_p04))\n",
        "# print(timecheck(apcp_train_c00, apcp_val_c00, apcp_test_c00))\n",
        "\n",
        "# print(timecheck(pwat_train_p01, pwat_val_p01, pwat_test_p01))\n",
        "# print(timecheck(pwat_train_p02, pwat_val_p02, pwat_test_p02))\n",
        "# print(timecheck(pwat_train_p03, pwat_val_p03, pwat_test_p03))\n",
        "# print(timecheck(pwat_train_p04, pwat_val_p04, pwat_test_p04))\n",
        "# print(timecheck(pwat_train_c00, pwat_val_c00, pwat_test_c00))\n",
        "\n",
        "# print(timecheck(cape_train_c00, cape_val_c00, cape_test_c00))\n",
        "# print(timecheck(t2m_train_c00, t2m_val_c00, t2m_test_c00))\n",
        "# print(timecheck(cin_train_c00, cin_val_c00, cin_test_c00))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGq_C2IrW7Qu"
      },
      "outputs": [],
      "source": [
        "def transform_train(trainds, var):\n",
        "    if var == 'slp': # tp\n",
        "        scaler_train = MinMaxScaler()\n",
        "        X_one_col = trainds.slp.values.reshape([trainds.slp.values.shape[0]*trainds.slp.values.shape[1]*trainds.slp.values.shape[2], 1])\n",
        "        X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "        X_one_col_res = scaler_train.fit_transform(X_one_col) # scaler_train.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train.inverse_transform(X_one_col_res) -1 only\n",
        "        trainds.slp.values = X_one_col_res.reshape(trainds.slp.values.shape)\n",
        "        return scaler_train, trainds\n",
        "\n",
        "    if var == 'rhum': # tp\n",
        "        scaler_train = MinMaxScaler()\n",
        "        X_one_col = trainds.rhum.values.reshape([trainds.rhum.values.shape[0]*trainds.rhum.values.shape[1]*trainds.rhum.values.shape[2], 1])\n",
        "        X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "        X_one_col_res = scaler_train.fit_transform(X_one_col) # scaler_train.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train.inverse_transform(X_one_col_res) -1 only\n",
        "        trainds.rhum.values = X_one_col_res.reshape(trainds.rhum.values.shape)\n",
        "        return scaler_train, trainds\n",
        "\n",
        "    if var == 'shum':\n",
        "        scaler_train = MinMaxScaler()\n",
        "        X_one_col = trainds.shum.values.reshape([trainds.shum.values.shape[0]*trainds.shum.values.shape[1]*trainds.shum.values.shape[2], 1])\n",
        "        # X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "        X_one_col_res = scaler_train.fit_transform(X_one_col) # scaler_train.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train.inverse_transform(X_one_col_res) -1 only\n",
        "        trainds.shum.values = X_one_col_res.reshape(trainds.shum.values.shape)\n",
        "        return scaler_train, trainds\n",
        "\n",
        "    if var == 'tmp':\n",
        "        scaler_train = MinMaxScaler()\n",
        "        X_one_col = trainds.tmp.values.reshape([trainds.tmp.values.shape[0]*trainds.tmp.values.shape[1]*trainds.tmp.values.shape[2], 1])\n",
        "        # X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "        X_one_col_res = scaler_train.fit_transform(X_one_col) # scaler_train.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train.inverse_transform(X_one_col_res) -1 only\n",
        "        trainds.tmp.values = X_one_col_res.reshape(trainds.tmp.values.shape)\n",
        "        return scaler_train, trainds\n",
        "\n",
        "    if var == 'hgt':\n",
        "        scaler_train = MinMaxScaler()\n",
        "        X_one_col = trainds.hgt.values.reshape([trainds.hgt.values.shape[0]*trainds.hgt.values.shape[1]*trainds.hgt.values.shape[2], 1])\n",
        "        # X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "        X_one_col_res = scaler_train.fit_transform(X_one_col) # scaler_train.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train.inverse_transform(X_one_col_res) -1 only\n",
        "        trainds.hgt.values = X_one_col_res.reshape(trainds.hgt.values.shape)\n",
        "        return scaler_train, trainds\n",
        "\n",
        "    if var == 't2m':\n",
        "        scaler_train = MinMaxScaler()\n",
        "        X_one_col = trainds.t2m.values.reshape([trainds.t2m.values.shape[0]*trainds.t2m.values.shape[1]*trainds.t2m.values.shape[2], 1])\n",
        "        # X_one_col = np.log10(X_one_col+1) # 10**X_one_col - 1 to scale back\n",
        "        X_one_col_res = scaler_train.fit_transform(X_one_col) # scaler_train.inverse_transform(X_one_col_res) to scale back, or use 10**scaler_train.inverse_transform(X_one_col_res) -1 only\n",
        "        trainds.t2m.values = X_one_col_res.reshape(trainds.t2m.values.shape)\n",
        "        return scaler_train, trainds\n",
        "\n",
        "    else:\n",
        "        print('Error: var not found in function, please define')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn8OpPDVX-fE"
      },
      "source": [
        "transforming train, val and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9tsAYUaXTGe"
      },
      "outputs": [],
      "source": [
        "scaler_train_hgt850,hgt850_train = transform_train(hgt850_train,'hgt')\n",
        "scaler_train_hgt500,hgt500_train = transform_train(hgt500_train, 'hgt')\n",
        "scaler_train_slp,slp_train = transform_train(slp_train,'slp')\n",
        "scaler_train_rhum,rhum_train= transform_train(rhum_train, 'rhum')\n",
        "scaler_train_shum,shum_train = transform_train(shum_train, 'shum')\n",
        "scaler_train_temp,temp_train = transform_train(temp_train, 'tmp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p10CMR-Zjgz"
      },
      "outputs": [],
      "source": [
        "def transform_val_test(val_test, scaler_train, is_prec=True):\n",
        "    '''\n",
        "    Input (example): ds_val_apcp.tp, scaler_train, True/False\n",
        "    Output: Transformed validation/test XR data\n",
        "    If is_prec set to True, variable is precipitation\n",
        "    '''\n",
        "    if is_prec:\n",
        "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
        "        X_one_col = np.log10(X_one_col+1)\n",
        "        X_one_col_res = scaler_train.transform(X_one_col)\n",
        "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
        "        return val_test\n",
        "\n",
        "    else:\n",
        "        X_one_col = val_test.values.reshape([val_test.values.shape[0]*val_test.values.shape[1]*val_test.values.shape[2], 1])\n",
        "        X_one_col_res = scaler_train.transform(X_one_col)\n",
        "        val_test.values = X_one_col_res.reshape(val_test.values.shape)\n",
        "        return val_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAtIQgAtZngO"
      },
      "outputs": [],
      "source": [
        "slp_val = transform_val_test(slp_val.slp, scaler_train_slp,is_prec = False)\n",
        "rhum_val = transform_val_test(rhum_val.rhum, scaler_train_rhum,is_prec = False)\n",
        "shum_val = transform_val_test(shum_val.shum, scaler_train_shum,is_prec = False)\n",
        "temp_val = transform_val_test(temp_val.tmp, scaler_train_temp,is_prec = False)\n",
        "hgt850_val = transform_val_test(hgt850_val.hgt, scaler_train_hgt850,is_prec = False)\n",
        "hgt500_val = transform_val_test(hgt500_val.hgt, scaler_train_hgt500,is_prec = False)\n",
        "\n",
        "\n",
        "slp_test = transform_val_test(slp_test.slp, scaler_train_slp, is_prec =  False)\n",
        "rhum_test = transform_val_test(rhum_test.rhum, scaler_train_rhum, is_prec =  False)\n",
        "shum_test = transform_val_test(shum_test.shum, scaler_train_shum, is_prec =  False)\n",
        "temp_test = transform_val_test(temp_test.tmp, scaler_train_temp, is_prec =  False)\n",
        "hgt850_test = transform_val_test(hgt850_test.hgt, scaler_train_hgt850, is_prec =  False)\n",
        "hgt500_test = transform_val_test(hgt500_test.hgt, scaler_train_hgt500, is_prec =  False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC9zrCr6HnNl"
      },
      "outputs": [],
      "source": [
        "print(slp_train)\n",
        "shum_train =shum_train.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN5pTDPyX-fF"
      },
      "source": [
        "converting to .npy files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZvOoFXdk9EK"
      },
      "outputs": [],
      "source": [
        "\n",
        "slp_train_data  = slp_train['slp']\n",
        "slp_train_final = slp_train_data.values\n",
        "print(slp_train_final)\n",
        "print(slp_train_final.shape)\n",
        "\n",
        "\n",
        "\n",
        "hgt850_train_data = hgt850_train['hgt']\n",
        "hgt850_train_final = hgt850_train_data.values\n",
        "\n",
        "hgt500_train_data = hgt500_train['hgt']\n",
        "hgt500_train_final = hgt500_train_data.values\n",
        "\n",
        "temp_train_data = temp_train['tmp']\n",
        "temp_train_final = temp_train_data.values\n",
        "\n",
        "rhum_train_data = rhum_train['rhum']\n",
        "rhum_train_final = rhum_train_data.values\n",
        "\n",
        "shum_train_data = shum_train['shum']\n",
        "shum_train_final = shum_train_data.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwINmRkYQGth"
      },
      "outputs": [],
      "source": [
        "lst_train_ensemble = [hgt850_train_final, hgt500_train_final, slp_train_final,temp_train_final, rhum_train_final, shum_train_final]\n",
        "#print(lst_train_ensemble)\n",
        "X_train_ensemble = np.stack((lst_train_ensemble), axis = -1)\n",
        "print(X_train_ensemble)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6S0gUj0lNah"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANKKluXWX-fF"
      },
      "source": [
        "combining variables into 1 4D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA-f9XxsX-fF"
      },
      "outputs": [],
      "source": [
        "lst_train_ensemble = [hgt850_train_final, hgt500_train_final, slp_train_final, rhum_train_final, shum_train_final]\n",
        "\n",
        "X_train_ensemble = np.stack((lst_train_ensemble), axis = -1) # stacking 13 variables into 1 single 4D array\n",
        "\n",
        "# lst_val_ensemble = [hgt850_val, hgt500_val, slp_train,temp_val, rhum_val, shum_val]\n",
        "\n",
        "# X_val_ensemble = np.stack((lst_val_ensemble), axis = -1) # stacking 13 variables into 1 single 4D array\n",
        "\n",
        "# lst_test_ensemble = [hgt850_test, hgt500_test, slp_test,temp_test, rhum_test, shum_test]\n",
        "\n",
        "# X_test_ensemble = np.stack((lst_test_ensemble), axis = -1) # stacking 13 variables into 1 single 4D array\n",
        "\n",
        "print(X_train_ensemble.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yagbk3u4X-fF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        " # please change this to your own path\n",
        "np.save('X_train_ensemble.npy', X_train_ensemble)\n",
        "\n",
        " np.save('X_val_ensemble.npy', X_val_ensemble)\n",
        " np.save('X_test_ensemble.npy', X_test_ensemble)\n",
        "\n",
        " files.download('X_train_ensemble.npy')\n",
        " files.download('X_val_ensemble.npy')\n",
        " files.download('X_test_ensemble.npy')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('research')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "287ad1a523a1a0c79ba273026018d677ef26436c7e0e825f5e9b183804324b70"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}